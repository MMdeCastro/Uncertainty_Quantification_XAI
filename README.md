
[![NBViewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.org/github/MMdeCastro/Uncertainty_Quantification_XAI/blob/main/Exploration_and_Classification.ipynb) Exploraci√≥n y Clasificaci√≥n

[![NBViewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.org/github/MMdeCastro/Uncertainty_Quantification_XAI/blob/main/XAI.ipynb) Explicabilidad

# Exactitud con certidumbre

_Taller para la [PyConES22](https://2022.es.pycon.org/) viernes 30 de Septiembre de 2022 de 15:30h a 17:30h._

Medir la calidad de las predicciones de un modelo de aprendizaje supervisado a trav√©s de m√©tricas de rendimiento como *accuracy* (o "exactitud" en castellano), *precision*, *recall*, *F1 score*,... no nos da la seguridad de que el modelo est√© respondiendo a la pregunta correcta, sobre todo si es un modelo no interpretable por las personas, tambi√©n llamados modelos 'black box' o caja negra. 

Un ejemplo t√≠pico son los grandes modelos de deep learning fallando al intentar identificar una vaca en la playa. Esto sucede frecuentemente porque lo que en realidad aprendi√≥ el modelo durante su entrenamiento fue a reconocer la hierba en las im√°genes del training set. Son modelos muy exactos (puede que hasta obtengamos m√°s de un 95% de *accuracy* en su validaci√≥n y testado), pero en una tarea distinta, no deseada (quer√≠amos detectar vacas, no hierba). Si solamente nos fijamos en las m√©tricas, este inesperado cambio de tarea puede pasar f√°cilmente desapercibido y aumentar el tama√±o del training set no necesariamente soluciona el problema.

Podemos cerciorarnos de que esto no nos pasa incorporando a nuestro modelo herramientas matem√°ticas (ya desarrolladas como librer√≠as de Python) que nos facilitan ir m√°s all√° de la optimizaci√≥n de las m√©tricas. Estas herramientas sirven para cualquier modelo supervisado y explican en qu√© se fij√≥ el modelo para producir sus predicciones (gracias a m√©todos de explicabilidad o [XAI](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence), como los SHAP values, LIME,...) o proveen de intervalos de confianza a las predicciones puntuales (utilizando m√©todos de cuantificaci√≥n de incertidumbre o [UQ](https://en.wikipedia.org/wiki/Uncertainty_quantification), como los Conformal Predictors, la Quantile Regression,..).

Este taller complementa a la mayor√≠a de los cursos introductorios sobre aprendizaje autom√°tico supervisado, que a menudo se centran solamente en optimizar m√©tricas. Si no has realizado un curso introductorio sobre aprendizaje autom√°tico, encontrar√°s un resumen en el Jupyter Notebook llamado `Exploration_and_Classification.ipynb`.

El an√°lisis completo es una serie de 3 Jupyter Notebooks, en este orden:

+ `Exploration_and_Classification.ipynb` donde exploramos y preparamos los datos, probamos varios modelos de aprendizaje autom√°tico supervisado para clasificaci√≥n, explicamos las m√©tricas de rendimiento, y elegimos el modelo que nos da mejor rendimiento (en el taller de la PyConES este Jupyter Notebook se da por entendido),
+ `XAI.ipynb` donde aplicamos varios m√©todos de explicabilidad [XAI](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence) (en el taller de la PyConES22 comenzamos por este Jupyter Notebook), y
+ `UQ.ipynb` donde aplicamos varios m√©todos de cuantificaci√≥n de incertidumbre [UQ](https://en.wikipedia.org/wiki/Uncertainty_quantification) (en el taller de la PyConES22 terminaremos con este Jupyter Notebook).

Como dec√≠amos, √©sta es una serie introductoria y no incluimos explicaciones exhaustivas y demostraciones matem√°ticas (hay muchas otras fuentes, ver la lista de materiales en la intro de los Jupyter Notebooks), mejor mencionaremos algunas caracter√≠sticas intuitivas sobre la explicabiliad y la cuantificaci√≥n de la incertidumbre y nos enfocaremos en su implementaci√≥n en Scikit-learn.

<font size="5"> üëçü§ì </font>

## Instrucciones

0. Clona este repositorio en el ordenador o nube donde vayas a trabajar. Si no tienes el paquete `git` instalado, [aqu√≠](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git) explica c√≥mo hacerlo. Para clonar el repositorio, pincha en el bot√≥n verde que dice 'Code', copia al portapapeles la direcci√≥n htpps, y p√©gala en una terminal donde vayas a trabajar, escribiendo `git clone`, un espacio, y la direcci√≥n htpps de este repositorio, tardar√° unos segundos en descargarse.  

1. Si no tienes el administrador de paquetes `conda` instalado, puedes simplemente instalar `miniconda` siguiendo las instrucciones para tu sistema operativo [aqu√≠](https://docs.conda.io/en/latest/miniconda.html). Despu√©s, abre una terminal (de Bash si est√°s en Linux o Mac, Anaconda prompt si est√°s en Windows) y escribe:

+ `conda env create -f environment.yml`

2. Activa el entorno escribiendo en la terminal:

+ `conda activate pycones22_UQ_XAI`

  La primera vez tardar√° bastante, dependiendo de tu ancho de banda, puede que hasta una hora, porfa, tr√°elo ya hecho cuando vengas al taller. Cuando acabes con este proyecto, desactiva el entorno escribiendo en la terminal:

+ `conda deactivate`

3. Abre la aplicaci√≥n para editar y ejecutar el c√≥digo en Jupyter Notebooks:

+ `jupyter notebook`

4. Jupyter se abrir√° en tu browser. En la barra de herramientas, pincha en 'Nbextensions' y permite 'Collapsible Headings' para mejorar la navegaci√≥n, es un Jupyter Notebook un poco largo! Abre el Jupyter Notebook clicando en un fichero con extensi√≥n .ipynb y sigue las instrucciones escritas all. Aconsejamos empezar por `Exploration_and_Classification.ipynb`.

De hecho el an√°lisis completo incluye los siguientes 3 notebooks, en este orden:

+ `Exploration_and_Classification.ipynb` donde exploramos y preparamos los datos, probamos varios modelos de aprendizaje autom√°tico supervisado para clasificaci√≥n, explicamos las m√©tricas de rendimiento, y elegimos el modelo que nos da mejor rendimiento (este Jupyter notebook se da por entendido y no se tratar√° en el taller de la PyConES22),
+ `XAI.ipynb` donde aplicamos varios m√©todos de explicabilidad [XAI](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence), y
+ `UQ.ipynb` donde aplicamos varios m√©todos de cuantificaci√≥n de incertidumbre [UQ](https://en.wikipedia.org/wiki/Uncertainty_quantification).


