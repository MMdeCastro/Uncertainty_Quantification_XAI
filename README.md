## M√°s all√° de la m√©trica

Este taller complementa a la mayor√≠a de los cursos introductorios sobre aprendizaje autom√°tico (machine learning) supervisado, que a menudo se centran solamente en optimizar m√©tricas de rendimiento. Si no has realizado un curso introductorio sobre aprendizaje autom√°tico supervisado, encontrar√°s un resumen en el Jupyter Notebook llamado `Exploration_and_Classification.ipynb`.

El an√°lisis completo es una serie de 3 Jupyter Notebooks, en este orden:

+ `Exploration_and_Classification.ipynb` donde exploramos y preparamos los datos, probamos varios modelos de aprendizaje autom√°tico supervisado para clasificaci√≥n, explicamos las m√©tricas de rendimiento y el umbral de decisi√≥n, y elegimos el modelo que nos da mejor rendimiento,
+ `XAI.ipynb` donde explicamos m√©todos de explicabilidad o [XAI](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence), y
+ `UQ.ipynb` donde explicamos m√©todos de cuantificaci√≥n de incertidumbre o [UQ](https://en.wikipedia.org/wiki/Uncertainty_quantification).

Para profundizar en la aplicaci√≥n de la UQ, ver el cuarto Jupyter Notebook `UQ_multiclass.ipynb` donde se incluye el c√≥digo para la explicaci√≥n de este [mini curso de Christoph Molnar](https://mindfulmodeler.substack.com/p/week-1-getting-started-with-conformal).

Si prefieres solo mirar el contenido sin ejecutarlo, aqu√≠ tienes los enlaces a cada uno de los temas:

[![NBViewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.org/github/MMdeCastro/Uncertainty_Quantification_XAI/blob/main/Exploration_and_Classification.ipynb) Exploraci√≥n de los datos y selecci√≥n del modelo de aprendizaje supervisado para la Clasificaci√≥n

[![NBViewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.org/github/MMdeCastro/Uncertainty_Quantification_XAI/blob/main/XAI.ipynb) Herramientas de Explicabilidad (XAI)

[![NBViewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.org/github/MMdeCastro/Uncertainty_Quantification_XAI/blob/main/UQ.ipynb) Herramientas de Cuantificaci√≥n de Incertidumbre (UQ)

[![NBViewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.org/github/MMdeCastro/Uncertainty_Quantification_XAI/blob/main/UQ_multiclass.ipynb) Conformal Predictors (CP) para la UQ en ejemplo de clasificaci√≥n multiclase

y el enlace a las diapositivas de las presentaciones introductorias (contienen animaciones, mejor verlas en modo presentaci√≥n):
+ CITIC - UGR ["Accuracy with Certainty: beyond the performance metrics"](https://docs.google.com/presentation/d/1pXtkpzjTj94vxgBhRlsuSFIwFkx8v53u8ZP_9lZDyi4/edit?usp=sharing),
+ DyploFest24 ["Exactitud con Certidumbre"](https://docs.google.com/presentation/d/1f2LiOLh_IQfKqGiJ1X8OIee0wxKJQZiKft5riam9UA8/edit#slide=id.g2de3d8b8587_0_17),
+ para el alumnado del M√°ster en Ciencia de Datos de la Universidad de Granada ["Introducci√≥n a la Cuantificaci√≥n de Incertidumbre en Aprendizaje Autom√°tico con Conformal Prediction"](https://docs.google.com/presentation/d/1yFHn4_Byt6_f5arFOdhUWBXOrN7BrYbmEc2gMhJ0RVY/edit?usp=sharing),
+ II Jornadas de Ciencia y G√©nero del IFIC y la Univ. de Valencia, charla para todos los p√∫blicos ["La IA no hereda nuestros sesgos si le ense√±amos c√≥mo"](https://docs.google.com/presentation/d/1bp8rJTtZ5aAGeNwwdTdue4vGcs27QvcEeA1VbHPce9c/edit#slide=id.g211627b4636_0_101). [Aqu√≠ est√° el v√≠deo](https://youtu.be/89G74PBnoVc) de la presentaci√≥n,
+ para estudiantes de inform√°tica y telecomunicaciones del Google Developer Student Club de la Univ. de Granada ["Herramientas para la explicabilidad y la cuantificaci√≥n de incertidumbre contra los sesgos del aprendizaje autom√°tico"](https://docs.google.com/presentation/d/1p5QVf4JaDDFl7IM6XLclfWbcU0a5a8Il9MTu8-9fB84/edit#slide=id.g20f7f9abf3c_0_101),
+ Python Granada ["Python para una IA responsable"](https://docs.google.com/presentation/d/1kil2P6pYuKcan1QcvaxNWXoyoEXkJnBWgpt0gZMfejg/edit?usp=sharing),
+ sobre los Conformal Predictors:
  + [Intro general](https://docs.google.com/presentation/d/1Q6oxcgmNv0GsmNFA5npAzQGDuy_XolBKStN4KUhP4Gk/edit?usp=sharing) (in English).
  + [Key Concepts of Conformal Prediction](https://docs.google.com/presentation/d/1bQYIFyQysQPx79wJq1mltylsH_aeO2AnLYRlHiLq6vo/edit#slide=id.g2dc32e8043a_0_5) (in English).

Como dec√≠amos, √©sta es una serie introductoria y no incluimos explicaciones exhaustivas ni demostraciones matem√°ticas (hay muchas otras fuentes, ver la lista de materiales en la intro de los Jupyter Notebooks), mejor mencionaremos algunas caracter√≠sticas intuitivas sobre la explicabiliad y la cuantificaci√≥n de la incertidumbre y nos enfocaremos en su implementaci√≥n en Scikit-learn (una de las librer√≠as de [Python](https://es.wikipedia.org/wiki/Python) m√°s utilizadas en aprendizaje autom√°tico) y paquetes compatibles.

# Exactitud con certidumbre

En inteligencia artificial, medimos la calidad de las predicciones de un modelo de aprendizaje autom√°tico supervisado a trav√©s de m√©tricas de rendimiento. Por ejemplo, la m√©trica de exactitud (accuracy) en una tarea de clasificaci√≥n cuenta cu√°ntas veces el modelo nos di√≥ la soluci√≥n correcta. El error cuadr√°tico medio (MSE) en una tarea de regresi√≥n mide a qu√© distancia se qued√≥ el modelo de la soluci√≥n correcta. Sin embargo, las m√©tricas de rendimiento no nos ofrecen certeza sobre las predicciones del modelo y esto es un gran problema, sobre todo si es un modelo no interpretable por las personas, tambi√©n llamados modelos de caja opaca (‚Äòblack box models‚Äô).

Un ejemplo es el de un modelo utilizado para clasificar radiograf√≠as de t√≥rax seg√∫n la gravedad de una neumon√≠a. Hab√≠a detectado una marca de la placa cuando la radiograf√≠a se hab√≠a realizado en la planta UCI y ese factor circunstancial era el que primaba en su predicci√≥n  ([_"AI for radiographic COVID-19 detection selects shortcuts over signal"_](https://www.nature.com/articles/s42256-021-00338-7)). Eran un modelo muy exacto, con una m√©trica de exactitud de m√°s del un 90% de aciertos, pero en una tarea distinta, no deseada (quer√≠amos detectar una enfermedad, no de d√≥nde ven√≠a la radiograf√≠a). El modelo simplemente encontr√≥ una correlaci√≥n espuria y cogi√≥ el atajo. Y sin poder avisarnos, arroj√≥ predicciones sesgadas.

Como segundo ejemplo imaginemos otro modelo clasificador, esta vez entrenado para predecir si llueve (clase 1) o no llueve (clase 0) dada la temperatura y la humedad del d√≠a. Si cuando lo ponemos en producci√≥n el resultado de un d√≠a es ‚Äú0.8‚Äù, muchos libros, cursos,‚Ä¶ concluyen que eso significa que ‚Äúhay un 80% de probabilidad de que llueva ese d√≠a y un 20% de probabilidad de que no llueva ese d√≠a‚Äù. Sin embargo, esos resultados no est√°n calibrados, es decir, no tienen en cuenta cu√°ntos d√≠as llovi√≥ de verdad, y por tanto no son verdaderas probabilidades. Pero la cosa empeora, ¬øqu√© ocurre si lo que le mostramos al modelo son los datos de un d√≠a con temperatura y humedad muy distintas a las que se usaron en su entrenamiento? Por ejemplo, las de un d√≠a en el planeta Venus. El modelo no tiene manera de decirnos ‚Äúno tengo ni idea de qu√© es esto‚Äù, simplemente arrojar√° un resultado, que en el mejor de los casos estar√° cerca de ‚Äú0.5‚Äù pero no hay ninguna garant√≠a de que as√≠ sea. Y se quedar√° tan pancho, porque no le hemos dado herramientas para mostrar la incertidumbre.

Al automatizar predicciones, si solamente nos fijamos en las m√©tricas de rendimiento, estos problemas pueden pasar f√°cilmente desapercibidos y aumentar el tama√±o del training set no necesariamente los soluciona. Podemos cerciorarnos de que esto no nos pasa incorporando a nuestro modelo ya entrenado ciertas herramientas sencillas para que pueda indicarnos cu√°ndo su predicci√≥n es fiable y cu√°ndo no. Estas herramientas matem√°ticas (ya desarrolladas como librer√≠as de Python y R) explican en qu√© se fij√≥ el modelo para producir sus predicciones (gracias a m√©todos de explicabilidad o ‚ÄúXAI‚Äù, como los SHAP values, LIME,‚Ä¶) o nos proveen de probabilidades calibradas e intervalos de predicci√≥n que incluyen con certeza el valor verdadero para que el modelo pueda avisarnos cuando no debemos confiar en su predicci√≥n (gracias a m√©todos de cuantificaci√≥n de incertidumbre o ‚ÄúUQ‚Äù, como los Conformal Predictors). Y todas son herramientas post-hoc (no hay que volver a entrenar el modelo), model-agnostic (sirven para cualquier modelo y tarea), y muy ligeras (se implementan con pocas l√≠neas de c√≥digo y se ejecutan muy r√°pido).

<font size="10"> üëçü§ì </font>

## Instrucciones

0. Clona este repositorio en el ordenador o nube donde vayas a trabajar. Si no tienes el paquete `git` instalado, [aqu√≠](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git) explica c√≥mo hacerlo. Para clonar el repositorio, pincha en el bot√≥n verde que dice 'Code', copia al portapapeles la direcci√≥n htpps, y p√©gala en una terminal donde vayas a trabajar, escribiendo `git clone`, un espacio, y la direcci√≥n htpps de este repositorio, tardar√° unos segundos en descargarse.  

1. Si no tienes el administrador de paquetes `conda` instalado, puedes simplemente instalar `miniconda` siguiendo las instrucciones para tu sistema operativo [aqu√≠](https://docs.conda.io/en/latest/miniconda.html). Al instalar `miniconda` se instala `Python` tambi√©n, viene incluido. Despu√©s, abre una terminal (de Bash si est√°s en Linux o Mac, Anaconda Prompt si est√°s en Windows) y escribe:

+ `conda env create -f environment.yml`

  ‚ö†Ô∏è Crear el entorno por primera vez puede tardar unos 10 minutos pero puede que m√°s, porfa, tr√°elo ya hecho cuando vengas al taller. 

2. Activa el entorno escribiendo en la terminal:

+ `conda activate intro_UQ_XAI`

  Cuando acabes con este proyecto, desactiva el entorno escribiendo en la terminal:

+ `conda deactivate`

3. Con el entorno activo, vamos a instalar los paquetes de XAI `LIME` y `SHAP` y el paquete de UQ `MAPIE` que es mejor instalarlos via `pip` en lugar de usar `conda`, tardar√°n unos pocos minutos en instalarse, para ello escribe en la terminal

+ `python3 -m pip install lime shap MAPIE`

4. Siempre con el entorno activo, abre la aplicaci√≥n de Jupyter para editar y ejecutar el c√≥digo en Jupyter Notebooks escribiendo en la terminal:

+ `jupyter notebook`

Jupyter se abrir√° en el browser que tengas por defecto (Firefox, Chrome,...). Los Jupyter Notebooks son los ficheros con extensi√≥n .ipynb, se abren clicando en ellos. Sigue las instrucciones escritas all√≠. Aconsejamos empezar por `Exploration_and_Classification.ipynb`. 

<font size="10"> üìù </font>Este taller se realiz√≥ por primera vez en la [PyConES22](https://2022.es.pycon.org/) viernes 30 de Septiembre de 2022 de 15:30h a 17:30h. En el taller de la PyConES22 damos este Jupyter Notebook por sabido y empezamos directamente con `XAI.ipynb`. Todas las charlas de la PyConES 2022 que se mencionan como material complementario en los Jupyter Notebooks pueden encontrarse en [la lista de reproduci√≥n del canal de youtube de Python Espana](https://www.youtube.com/@PythonES). 
